{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aed66d5",
   "metadata": {},
   "source": [
    "2025-11-13 \n",
    "Seq2Seq(Sequence-to-Sequence) 문제 정의와 해결 전략 이해\n",
    "기본 순환신경망(RNN) 한계와 LSTM 개선 아이디어 파악\n",
    "Encoder-Decoder 구조의 정보 흐름(컨텍스트 벡터/숨겨진 상태) 이해\n",
    "Teacher Forcing 기법의 학습 안정화 역할 및 Trade-off 이해\n",
    "Softmax와 CrossEntropy 손실의 수식 및 직관적 해석\n",
    "Inference 단계(그리디 vs 빔 서치)의 실전 활용 전략\n",
    "전체 파이프라인을 하나의 흐름으로 연결하여 종합 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3ea3f",
   "metadata": {},
   "source": [
    "seq2seq\n",
    "```\n",
    "영문장을 한국어로 번역, 질의 --> 답변생성, 문장요약 등.\n",
    "why?\n",
    "    입력과 출력의 길이가 다를 수 있는 문제(번역, 요약, 챗봇)\n",
    "    고정크기의 피쳐벡터는 순서정보와 문맥 관계를 충분히 반영못함\n",
    "    단순 분류모델은 시퀀스 간 종속적 생성(토큰별 점진적 예측)에 부적합\n",
    "```\n",
    "동작원리\n",
    "```\n",
    "    1.Encoder : 입력토큰들을 순차적으로 처리-> 마지막/전체 Hidden State 집약\n",
    "    2.Context(요약표현) :인코더의 정보를 압축\n",
    "    3.Decoder : Context + 이전에 생성한 토큰을 이용해서 다음 토큰을 반복생성\n",
    "    4.종료 : 특별한 EOS 토큰이 나올때까지 \n",
    "```\n",
    "실제사용예시\n",
    "```\n",
    "    번역, 대화시스템,문서요약,코드자동생성\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b33eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
