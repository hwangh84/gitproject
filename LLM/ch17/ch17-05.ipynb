{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18037acf",
   "metadata": {},
   "source": [
    "- 개체명 인식 : NER\n",
    "    - 텍스트에서 특정 의미를 가진 단어나 구절을 찾아내고 분류하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225ed956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 홍길동은 2025년 11월 19일 서울시청에서 삼성전자 직원을 만났다.\n",
    "# 홍길동 - [인명]\n",
    "# 2025년 11월 19일 - [일시]\n",
    "# 서울시청 - [지명]\n",
    "# 삼성전자 - [ 기관명]\n",
    "\n",
    "# 활용분야\n",
    "    #뉴스기사 : 기사에서 인물, 장소, 기관 자동추출\n",
    "    # 의료문서 : 병명, 약물명, 증상\n",
    "    # 계약서 : 회사명, 날자, 금액\n",
    "    # 챗봇 : 사용자 질문에 핵심정보 파악\n",
    "# BIO 태깅\n",
    "# B(begin) 개체 시작\n",
    "# I(inside) 개체 내부\n",
    "# O(outside) 개체가 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cbffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73031ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는         | B-PER    | 'PER' 개체의 시작\n",
      "2024년        | B-DAT    | 'DAT' 개체의 시작\n",
      "1월           | I-DAT    | 'DAT' 개체의 내부\n",
      "15일          | I-DAT    | 'DAT' 개체의 내부\n",
      "서울시청에서       | B-LOC    | 'LOC' 개체의 시작\n",
      "삼성전자         | B-ORG    | 'ORG' 개체의 시작\n",
      "직원을          | O        | 개체가 아님\n",
      "만났다          | O        | 개체가 아님\n"
     ]
    }
   ],
   "source": [
    "# Bio 태깅\n",
    "tokens = [\"김철수는\", \"2024년\", \"1월\", \"15일\", \"서울시청에서\", \"삼성전자\", \"직원을\", \"만났다\"]\n",
    "bio_tags = [\"B-PER\", \"B-DAT\", \"I-DAT\", \"I-DAT\", \"B-LOC\", \"B-ORG\", \"O\", \"O\"]\n",
    "for token, tag in zip(tokens, bio_tags):\n",
    "  if tag.startswith('B-'):\n",
    "    desc = f\"'{tag[2:]}' 개체의 시작\"\n",
    "  elif tag.startswith('I-'):\n",
    "    desc = f\"'{tag[2:]}' 개체의 내부\"\n",
    "  else:\n",
    "    desc = \"개체가 아님\"\n",
    "  print(f\"{token:12} | {tag:8} | {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92024c2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df380f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터\n",
    "train_sentences = [\n",
    "    [\"김철수는\", \"서울에\", \"산다\"],\n",
    "    [\"이영희는\", \"2024년에\", \"부산으로\", \"이사했다\"],\n",
    "    [\"삼성전자는\", \"대한민국의\", \"대기업이다\"],\n",
    "    [\"박지성은\", \"축구선수다\"],\n",
    "    [\"2025년\", \"1월\", \"1일은\", \"새해다\"],\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    [\"B-PER\", \"B-LOC\", \"O\"],\n",
    "    [\"B-PER\", \"B-DAT\", \"B-LOC\", \"O\"],\n",
    "    [\"B-ORG\", \"B-LOC\", \"O\"],\n",
    "    [\"B-PER\", \"O\"],\n",
    "    [\"B-DAT\", \"I-DAT\", \"I-DAT\", \"O\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881defd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf==3.20.3 in c:\\users\\khh11\\miniconda3\\envs\\pyt_env\\lib\\site-packages (3.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547150a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[517, 490, 494,   0, 517,   0, 491,   0, 491,   0, 517,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import mod\n",
    "# 토크나이져\n",
    "MODEL_NAME = 'skt/kobert-base-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "text = '김철수는 서울에 산다'\n",
    "#토크나이져\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# 인코딩\n",
    "encoded = tokenizer(text,return_tensors='pt')\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ad9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER 모델 3단계로 구성\n",
    "# 1. 입력 텍스트\n",
    "# 2. koBERT 인코더\n",
    "# 3. 분류기(Linear)\n",
    "# 4. 출력 라벨 B-PER B-LOC B-ORG I-PER I-LOC I-ORG O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d0f67d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_labels:\n\u001b[32m      4\u001b[39m     results.append(i)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(results.shape)\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "results = []\n",
    "for i in train_labels:\n",
    "    results.append(i)\n",
    "results = np.array(results)\n",
    "print(results.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c77fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 문장 김철수는 서울에 산다\n",
      "정답라벨 B-PER B-LOC O\n",
      "김철수는       -> O        정답 : B-PER\n",
      "김철수는       -> B-PER    정답 : B-PER\n",
      "김철수는       -> O        정답 : B-PER\n",
      "김철수는       -> O        정답 : B-PER\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> B-PER    정답 : B-LOC\n",
      "서울에        -> B-PER    정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "서울에        -> O        정답 : B-LOC\n",
      "산다         -> O        정답 : O\n",
      "산다         -> O        정답 : O\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# SimpleNERModel 클래스 정의: Named Entity Recognition(개체명 인식)을 위한 간단한 모델\n",
    "# nn.Module을 상속받아 PyTorch 모델로 작동합니다.\n",
    "class SimpleNERModel(nn.Module):\n",
    "  # 모델 초기화 메서드\n",
    "  # num_labels: 개체명 라벨(클래스)의 총 개수\n",
    "  def __init__(self, num_labels) -> None:\n",
    "    # 부모 클래스인 nn.Module의 생성자를 호출하여 초기화합니다.\n",
    "    super(SimpleNERModel, self).__init__()\n",
    "    # 라벨의 개수를 인스턴스 변수로 저장합니다.\n",
    "    self.num_labels = num_labels\n",
    "    # 사전 학습된 BERT 모델을 로드합니다. (MODEL_NAME은 외부에서 정의되어야 합니다.)\n",
    "    # 이 BERT 모델은 입력 토큰을 임베딩하고 문맥 정보를 학습합니다.\n",
    "    self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    # 드롭아웃 레이어를 정의합니다. 과적합을 방지하기 위해 0.1의 확률로 뉴런을 비활성화합니다.\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    # 분류를 위한 선형 레이어(Fully Connected Layer)를 정의합니다.\n",
    "    # BERT 모델의 출력 은닉 상태 크기를 입력으로 받고, 라벨 개수를 출력으로 가집니다.\n",
    "    self.clf = nn.Linear(self.bert.config.hidden_size,  self.num_labels)\n",
    "\n",
    "  # 모델의 순전파(forward pass)를 정의하는 메서드\n",
    "  # input_ids: 입력 토큰 ID 시퀀스\n",
    "  # attention_mask: 어텐션 마스크 (패딩 토큰을 무시하도록 합니다)\n",
    "  def forward(self, input_ids, attention_maks):\n",
    "    # BERT 모델을 사용하여 입력 시퀀스를 인코딩합니다.\n",
    "    # outputs에는 last_hidden_state, pooler_output 등 다양한 정보가 포함됩니다.\n",
    "    outputs = self.bert(input_ids, attention_mask=attention_maks)\n",
    "    # BERT의 마지막 은닉 상태(hidden state)를 추출합니다.\n",
    "    # 이 상태는 각 입력 토큰에 대한 문맥적 임베딩을 포함합니다.\n",
    "    sequence_output =  outputs.last_hidden_state\n",
    "    # 추출된 은닉 상태에 드롭아웃을 적용합니다.\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "    # 드롭아웃이 적용된 은닉 상태를 분류기(선형 레이어)에 통과시켜 로짓(logit)을 계산합니다.\n",
    "    # 로짓은 각 라벨에 대한 분류 점수를 나타냅니다.\n",
    "    logits = self.clf(sequence_output)\n",
    "    # 계산된 로짓을 반환합니다.\n",
    "    return logits\n",
    "\n",
    "# 라벨의 고유한 목록을 생성합니다.\n",
    "# train_labels는 각 샘플의 라벨 시퀀스를 포함하는 리스트의 리스트 형태일 것으로 예상됩니다.\n",
    "label_list = set([data for i in train_labels for data in i])\n",
    "# 생성된 라벨 목록을 출력합니다. (디버깅 또는 확인용)\n",
    "label_list\n",
    "# SimpleNERModel 인스턴스를 생성합니다.\n",
    "# 모델의 num_labels는 고유한 라벨의 개수로 설정됩니다.\n",
    "model = SimpleNERModel(num_labels=len(label_list))\n",
    "\n",
    "# 모델 학습\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 순전파 테스트\n",
    "# 순전파 테스트를 위한 코드 블록 시작\n",
    "\n",
    "# 학습 데이터셋에서 첫 번째 문장과 해당 라벨을 샘플로 가져옵니다.\n",
    "# train_sentences는 문장들의 리스트이고, 각 문장은 단어들의 리스트로 구성됩니다.\n",
    "sample_sentence = train_sentences[0]\n",
    "# train_labels는 라벨 시퀀스들의 리스트이고, 각 라벨 시퀀스는 단어별 라벨들의 리스트로 구성됩니다.\n",
    "sample_label = train_labels[0]  \n",
    "\n",
    "# 샘플 문장과 그에 해당하는 정답 라벨을 출력하여 확인합니다.\n",
    "# ' '.join()을 사용하여 단어 리스트를 공백으로 구분된 하나의 문자열로 만듭니다.\n",
    "print(f\"테스트 문장 {' '.join(sample_sentence)}\")\n",
    "print(f\"정답라벨 {' '.join(sample_label)}\")\n",
    "\n",
    "# 토크나이저를 사용하여 샘플 문장을 인코딩합니다.\n",
    "# return_tensors=\"pt\": PyTorch 텐서 형식으로 반환하도록 지정합니다.\n",
    "# truncation=True: 최대 길이를 초과하는 시퀀스를 자릅니다.\n",
    "# max_length=32: 시퀀스의 최대 길이를 32로 설정합니다.\n",
    "# padding=True: 모든 시퀀스를 max_length에 맞춰 패딩합니다.\n",
    "# is_split_into_words=True: 입력이 이미 단어 단위로 분리된 리스트임을 토크나이저에게 알려줍니다.\n",
    "encoding =tokenizer(sample_sentence, return_tensors=\"pt\", truncation=True, \n",
    "          max_length=32,padding=True, is_split_into_words=True)\n",
    "# 인코딩된 input_ids (토큰 ID)를 모델이 있는 장치(CPU 또는 GPU)로 이동시킵니다.\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "# 인코딩된 attention_mask (패딩 토큰을 무시하기 위한 마스크)를 모델이 있는 장치로 이동시킵니다.\n",
    "attention_masks = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# 모델 평가 모드 설정 및 순전파 수행\n",
    "# torch.no_grad(): 그래디언트 계산을 비활성화하여 메모리 사용량을 줄이고 계산 속도를 높입니다.\n",
    "#                  평가 단계에서는 역전파가 필요 없으므로 이 컨텍스트를 사용합니다.\n",
    "with torch.no_grad():\n",
    "    # 모델을 평가 모드로 설정합니다. (Dropout, BatchNorm 등의 동작이 평가 모드에 맞게 변경됩니다.)\n",
    "    model.eval()\n",
    "    # 모델의 forward 메서드를 호출하여 로짓(logit)을 계산합니다.\n",
    "    # 로짓은 각 토큰에 대한 각 라벨 클래스의 예측 점수입니다.\n",
    "    logits = model(input_ids, attention_masks)\n",
    "    # 로짓에서 가장 높은 값을 가지는 인덱스를 찾아 예측 라벨로 결정합니다.\n",
    "    # dim=-1: 마지막 차원(클래스 차원)을 기준으로 argmax를 수행합니다.\n",
    "    predictions = torch.argmax(logits, dim=-1)  \n",
    "\n",
    "# 토크나이저의 word_ids 메서드를 사용하여 각 토큰이 원본 문장의 어떤 단어에 해당하는지 매핑합니다.\n",
    "# batch_index=0: 단일 샘플이므로 첫 번째 배치 인덱스를 사용합니다.\n",
    "word_ids = encoding.word_ids(batch_index=0)\n",
    "# 예측된 라벨들을 저장할 빈 리스트를 초기화합니다.\n",
    "pred_labels = []\n",
    "\n",
    "# 라벨 인덱스를 실제 라벨 문자열로 매핑하기 위한 딕셔너리를 생성합니다.\n",
    "# label_list는 고유한 라벨 문자열들의 리스트입니다.\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "# word_ids를 순회하며 각 토큰에 대한 예측 라벨을 원본 단어에 매핑합니다.\n",
    "for i, word_idx in enumerate(word_ids):\n",
    "    # word_idx가 None이 아니고 (특수 토큰이 아님) 예측 결과 범위 내에 있을 때만 처리합니다.\n",
    "    if word_idx is not None and i < len(predictions[0]):\n",
    "        # 현재 토큰의 예측된 라벨 인덱스를 가져와 id2label 딕셔너리를 통해 실제 라벨 문자열로 변환합니다.\n",
    "        pred_label = id2label[predictions[0][i].item()]\n",
    "        # 원본 문장의 단어 인덱스가 유효한 범위 내에 있을 때만 출력합니다.\n",
    "        if word_idx < len(sample_sentence):\n",
    "            # 원본 단어, 모델의 예측 라벨, 그리고 실제 정답 라벨을 함께 출력합니다.\n",
    "            # f-string 포매팅을 사용하여 출력 형식을 맞춥니다.\n",
    "            print(f\"{sample_sentence[word_idx]:10} -> {pred_label:8} 정답 : {sample_label[word_idx]}\")\n",
    "\n",
    "           \n",
    "# # 데이터 로더 정의\n",
    "# class NERDataset(Dataset):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
