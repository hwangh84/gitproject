{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213fff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mask : 실제토큰 1 / 0 패딩\n",
    "# Token Type IDS(Segment IDs): 두개의 문장(A/B) 구성될 때 각 토큰이 어느 문장에 속하는지 알려주는 임베딩\n",
    "# CLS Token Pooling : [CLS] + token + [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85b9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : Hello World\n",
      "토큰 : ['hello', 'world']\n",
      "ID : [7592, 2088]\n",
      "역변환 : hello world/n\n",
      "원문 : unbelievable performance!\n",
      "토큰 : ['unbelievable', 'performance', '!']\n",
      "ID : [23653, 2836, 999]\n",
      "역변환 : unbelievable performance!/n\n",
      "원문 : COVID-19 pendamic\n",
      "토큰 : ['co', '##vid', '-', '19', 'pen', '##dam', '##ic']\n",
      "ID : [2522, 17258, 1011, 2539, 7279, 17130, 2594]\n",
      "역변환 : covid - 19 pendamic/n\n"
     ]
    }
   ],
   "source": [
    "#  1. Bert Tokenizer : 단어를 의미있는 조각(subword)로 나눕니다. unbelievable \"un\" \"believ\" \"able\"\n",
    "from transformers import BertTokenizer\n",
    "# 토크나이져 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentences = [\n",
    "    'Hello World',\n",
    "    'unbelievable performance!',\n",
    "    'COVID-19 pendamic'\n",
    "]\n",
    "for sentence in sentences:\n",
    "    # 토큰화\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(f'원문 : {sentence}')\n",
    "    print(f'토큰 : {tokens}')\n",
    "\n",
    "    # ID 변환\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f'ID : {ids}')\n",
    "\n",
    "    # 역변환\n",
    "    decoded_string = tokenizer.decode(ids)\n",
    "    print(f'역변환 : {decoded_string}/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3ab9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_tensor': 'pt'} not recognized.\n",
      "Keyword arguments {'return_tensor': 'pt'} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2460, 6251, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2023, 2003, 1037, 2172, 2936, 6251, 2007, 2062, 2616, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Attention Mask : 실제단어 1 \n",
    "from transformers import BertTokenizer\n",
    "# 토크나이져 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentences = [\n",
    "    'short sentence',\n",
    "    'This is a much longer sentence with more words'\n",
    "]\n",
    "# 여러문장을 한꺼번에 토크나이징하고 가장 긴 문장길이에 맞춰 자동 패딩 수\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    return_tensor='pt'\n",
    ")\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "[CLS]                   101      0 (문장 A)\n",
      "the                    1996      0 (문장 A)\n",
      "weather                4633      0 (문장 A)\n",
      "is                     2003      0 (문장 A)\n",
      "nice                   3835      0 (문장 A)\n",
      "[SEP]                   102      0 (구분자)\n",
      "let                    2292      1 (문장 B)\n",
      "'                      1005      1 (문장 B)\n",
      "s                      1055      1 (문장 B)\n",
      "go                     2175      1 (문장 B)\n",
      "for                    2005      1 (문장 B)\n",
      "a                      1037      1 (문장 B)\n",
      "walk                   3328      1 (문장 B)\n",
      "[SEP]                   102      1 (구분자)\n"
     ]
    }
   ],
   "source": [
    "# token_type_ids : 두 문장을 입력할 때 첫번째, 두번째 구문\n",
    "from transformers import BertTokenizer\n",
    "# 토크나이져 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence_A = \"The weather is nice\"\n",
    "sentence_B = \"Let's go for a walk\"\n",
    "# 두 문장을 하나의 입력으로 인코딩\n",
    "encoded = tokenizer(\n",
    "    sentence_A,\n",
    "    sentence_B,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(encoded['token_type_ids'])\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "\n",
    "for token, token_id, type_id in zip(tokens, encoded['input_ids'][0], encoded['token_type_ids'][0]):\n",
    "    segment = \"문장 A\" if type_id == 0 else \"문장 B\"\n",
    "    if token == \"[SEP]\":\n",
    "        segment = \"구분자\"\n",
    "    elif token == \"[CLS]\":\n",
    "        segment = \"시작\"\n",
    "    print(f'{token:20s} {token_id.item():6d} {type_id.item():6d} ({segment})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : BERT is amazing for NLP tasks!\n",
      "last_hidden_state 형태 : torch.Size([1, 10, 768])\n",
      "batch_size = 1 sequence_length = 10 hidden_size = 768\n",
      "cls_embedding 형태 : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# [CLS] Token Pooling : BERT 첫번째 토큰 [CLS] 문서 전체의 요약 => 분류 작업을 할때\n",
    "# 이 토큰의 출력만 가져와서 분류기(classifier)에 연결\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "# 토크나이져 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "sentence = \"BERT is amazing for NLP tasks!\"\n",
    "# 인코딩\n",
    "inputs = tokenizer(sentence,return_tensors='pt')\n",
    "# BERT 통과\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# 출력 형태 확인\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "print(f'입력문장 : {sentence}')\n",
    "print(f'last_hidden_state 형태 : {last_hidden_state.shape}')\n",
    "print(f'batch_size = 1 sequence_length = {last_hidden_state.shape[1]} hidden_size = {last_hidden_state.shape[2]}')\n",
    "# [CLS] 토큰 추출\n",
    "cls_embedding = last_hidden_state[:,0,:]\n",
    "print(f'cls_embedding 형태 : {cls_embedding.shape}')\n",
    "# 분류기 (2-class)\n",
    "classifier = torch.nn.Linear(768, 2)\n",
    "logits = classifier(cls_embedding)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(f'logits : {logits}')\n",
    "print(f'probs : {probs}')\n",
    "print(f'predicted class : {torch.argmax(probs).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fc30a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss : 0.6073481440544128\n",
      "epoch : 2, loss : 0.661867082118988\n",
      "epoch : 3, loss : 0.6461204886436462\n",
      "epoch : 4, loss : 0.5061250329017639\n",
      "epoch : 5, loss : 0.3955150842666626\n",
      "epoch : 6, loss : 0.4086294621229172\n",
      "epoch : 7, loss : 0.35578736662864685\n",
      "epoch : 8, loss : 0.3439542055130005\n",
      "epoch : 9, loss : 0.3139413297176361\n",
      "epoch : 10, loss : 0.33423812687397003\n",
      "epoch : 11, loss : 0.28059518337249756\n",
      "epoch : 12, loss : 0.23552437126636505\n",
      "epoch : 13, loss : 0.2799035906791687\n",
      "epoch : 14, loss : 0.2156563103199005\n",
      "epoch : 15, loss : 0.2041623443365097\n",
      "epoch : 16, loss : 0.19555847346782684\n",
      "epoch : 17, loss : 0.22804558277130127\n",
      "epoch : 18, loss : 0.22621099650859833\n",
      "epoch : 19, loss : 0.1387973204255104\n",
      "epoch : 20, loss : 0.1997784674167633\n"
     ]
    }
   ],
   "source": [
    "# 미세 조정 학습 Fine-turning\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "texts = [\n",
    "    \"This movie is fantastic!\",\n",
    "    \"Terrible film, waste of time.\",\n",
    "    \"Amazing plot and great acting.\",\n",
    "    \"Boring and predictable.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# 토크나이져 모델\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 모델\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "# 데이터셋\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, texts, labels):\n",
    "    self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "    self.labels = labels\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "dataset = SimpleDataset(texts,labels)\n",
    "loader = DataLoader(dataset, batch_size=2)\n",
    "# 학습설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# 미세조정\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "  total_loss = 0\n",
    "  for batch in loader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs = { k:v.to(device) for k,v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "  print(f'epoch : {epoch+1}, loss : {total_loss/len(loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6172cca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     logits = outputs.logits \u001b[38;5;66;03m#batch, class number (1,2)\u001b[39;00m\n\u001b[32m     23\u001b[39m     probs = torch.softmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     pred = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# probs\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#     probs = torch.softmax(logits, dim=-1) # (1,2)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#     pred = torch.argmax(probs, dim=-1).item()\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(probs, pred)  \u001b[38;5;66;03m#1: positive 0: negative\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "model.eval()    #평가모드\n",
    "sample_sentences = [\n",
    "\"I am really disappointed with the result.\",\n",
    "\"The service was terrible and not worth the money.\",\n",
    "\"I don't like this product at all.\"\n",
    "]\n",
    "# 토큰화\n",
    "inputs = tokenizer(\n",
    "    sample_sentences,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "inputs\n",
    "# gpu/cpu 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits #batch, class number (1,2)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    pred = torch.argmax(probs, dim=-1).detach().numpy()\n",
    "    # probs\n",
    "#     probs = torch.softmax(logits, dim=-1) # (1,2)\n",
    "#     pred = torch.argmax(probs, dim=-1).item()\n",
    "    print(probs, pred)  #1: positive 0: negative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
