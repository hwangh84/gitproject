{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10adbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinormal Native Bayes 확률 모델\n",
    "# LogisticRegression : 다중클래스 회귀 기반 분류\n",
    "\n",
    "# RidgeClassifier : 회귀 기반 분류 L2규제\n",
    "\n",
    "# N-gram : 단점 차원폭발에 주의 (정규화/차원 축소 고려)\n",
    "\n",
    "# kolpy Okt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe2f8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 분류 모델\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9d5ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# data load\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "                    subset='train',\n",
    "                    remove=('headers', 'footers', 'quotes'),\n",
    "                    categories=categories\n",
    "                  )\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "                    subset='test',\n",
    "                    remove=('headers', 'footers', 'quotes'),\n",
    "                    categories=categories\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5a46401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_path = r'C:\\python_src\\LLM\\20newsbydate\\20news-bydate-train'\n",
    "test_path = r'C:\\python_src\\LLM\\20newsbydate\\20news-bydate-test'\n",
    "\n",
    "newsgroups_train = load_files(train_path, encoding='latin1')\n",
    "newsgroups_test  = load_files(test_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c57ef13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 제거\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = newsgroups_train.target_names\n",
    "    selected_idx = [target_names.index(c) for c in categories]\n",
    "\n",
    "    # 필터링\n",
    "    data_filtered, target_filtered = [],[]\n",
    "    for text, label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)   # 라벨 재정렬\n",
    "            data_filtered.append(text); target_filtered.append(new_label )\n",
    "    return data_filtered, target_filtered, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "391162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target, target_names = filter_categories(newsgroups_train, categories)\n",
    "test_data, test_target, _     = filter_categories(newsgroups_test, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1430ca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "511078ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤더 , 푸터 인용문 제거\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 헤더 제거\n",
    "    text = re.sub(r'^From:.\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 풋터 제거\n",
    "    text = re.sub(r'\\n--\\n.$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 인용문 제거\n",
    "    text = re.sub(r'(^|\\n)[>|:].', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1872dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [clean_text(t) for t in train_data]\n",
    "test_data  = [clean_text(t) for t in test_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9573cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 2034, 1353)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_target),len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850ef67",
   "metadata": {},
   "source": [
    "- 멀티노멀 나이즈베이즈\n",
    "- 문서에 포함된 단어들의 출현 횟수를 기반으로 해서 그 문서가 어떤 주제에 속할지 확률적\n",
    "- 스팸필터링, 뉴스기사 카테고리, 감성분석\n",
    "\n",
    "- 베이즈정리 확률 이론 - 조건부 확률\n",
    "- 단어A가 나왔을때 이 문서가 스팸 B 일 확률은 얼마\n",
    "\n",
    "$P(\\text{스팸} | \\text{단어들}) = \\frac{P(\\text{단어들} | \\text{스팸}) \\cdot P(\\text{스팸})}{P(\\text{단어들})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571ddce",
   "metadata": {},
   "source": [
    "- 나이브 Naive : 순진한 가정\n",
    "    - 가정 : 문서 안의 모든 단어는 서로 독립적\n",
    "    - 현실 : 스팸에 자주 나오는 단어들은 서로 독립적이지 않다.\n",
    "    - 실제 : 이러한 가정은 계산량을 빠르게 하고 단순하지만 정확도가 어느 정도 나온다.\n",
    "- 멀티노멀 : 다항분포\n",
    "    - 의미 : 단어의 출현 횟수를 중요\n",
    "    - 횟수를 세는 멀티노미얼 방식이 NLP 잘 맞는다.\n",
    "    - 모델은 단어의 빈도수 통계\n",
    "    - 스팸메일 통계\n",
    "        - free : 150\n",
    "        - money : 100\n",
    "        - viagra : 50\n",
    "        - report : 5\n",
    "    - 정상메일 Ham 통계\n",
    "        - report:80\n",
    "        - meeting : 60\n",
    "        - free : 10\n",
    "    - 이러한 통계를 바탕으로 이 카테고리에서 특정 단어가 나올 확률 P('free'| 스펨)을 모두 계산\n",
    "    - \"Free money meeting\"\n",
    "        - 스팸??\n",
    "            - 기본스팸확률 x, 스팸일 때 free가 나올 확률 x 스팸일 때 money가 나올 확룰 x 스팸일 때 meeting \n",
    "            나올 확률\n",
    "        - 정상\n",
    "            - 기본 정상확률 x 정상일 때...\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c027597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names\n",
    "\n",
    "# nltk tokenizer, \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d6f8df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034, 2000), (1353, 2000))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  min_df : 단어의 빈도가 최소 5개의 문서에 등장  - 노이즈 감소\n",
    "#  max_df : 50% 너무 흔한 단어는 제거\n",
    "\n",
    "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_cv = cv.fit_transform(train_data)\n",
    "x_test_cv  = cv.transform(test_data)\n",
    "\n",
    "x_train_cv.shape, x_test_cv.shape\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf74c46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '01', ..., 'zip', 'zoo', 'zoology'],\n",
       "      shape=(2000,), dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67882043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], shape=(2000,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv[0].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b76c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.80      0.79      0.80       319\n",
      "talk.religion.misc       0.74      0.73      0.73       251\n",
      "     comp.graphics       0.92      0.95      0.93       389\n",
      "         sci.space       0.93      0.91      0.92       394\n",
      "\n",
      "          accuracy                           0.86      1353\n",
      "         macro avg       0.85      0.85      0.85      1353\n",
      "      weighted avg       0.86      0.86      0.86      1353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW 기반 + MNB\n",
    "# 텍스트 분류의 강력한 baseline 희소데이터에 강함\n",
    "nb = MultinomialNB()\n",
    "# 학습용데이터 벡터데이터\n",
    "nb.fit(x_train_cv, train_target)\n",
    "nb.score(x_train_cv, train_target), nb.score(x_test_cv, test_target)\n",
    "\n",
    "# 분류 리포트\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_nb = nb.predict(x_test_cv)\n",
    "print(classification_report(test_target, y_pred_nb, target_names=categories) )\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08b13f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9513274336283186 0.38802660753880264\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + MNB + LogisticRegression\n",
    "# TF-IDF로 중요단어 강조, 선형모델과 자주사용 BOW 대비 흔한 단어 영향 감소\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(train_data)\n",
    "x_test_tfidf  = tfidf.fit_transform(test_data)\n",
    "\n",
    "# NB + tf-idf\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(x_train_tfidf, train_target)\n",
    "print(nb_tfidf.score(x_train_tfidf, train_target), nb_tfidf.score(x_test_tfidf, test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8514cc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9834050399508297 0.9828009828009828\n"
     ]
    }
   ],
   "source": [
    "# logistic\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test \\\n",
    "    = train_test_split(x_train_tfidf, train_target, test_size=0.2\n",
    "                       ,stratify=train_target, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train_tfidf, train_target)\n",
    "print(lr.score(x_train, y_train), lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9ab5317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9594345421020283 0.9336609336609336\n"
     ]
    }
   ],
   "source": [
    "# 과적합 해결을 위한 규제\n",
    "rc = RidgeClassifier(alpha=10)\n",
    "rc.fit(x_train, y_train)\n",
    "print(rc.score(x_train, y_train), rc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7cd06105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9373079287031346 0.914004914004914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# L1 규제 L1 Logistic(Lasso와 유사)\n",
    "# 일부 계수를 0으로 만들어서 특성 선택을 수행.. 중요피처 select 효과\n",
    "l1_lr = LogisticRegression(penalty='l1', max_iter=1000, solver='liblinear')\n",
    "l1_lr.fit(x_train, y_train)\n",
    "print(l1_lr.score(x_train, y_train), l1_lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "77f0ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리 모델 + tfidf\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "fores = RandomForestClassifier(\n",
    "    n_estimators=50,     # 트리 개수\n",
    "    max_depth=10,         # 최대 깊이\n",
    "    min_samples_split=2,  # 분할을 위한 최소 샘플 수\n",
    "    min_samples_leaf=1,   # 리프 노드의 최소 샘플 수\n",
    "    random_state=42       # 재현성을 위한 시드값\n",
    ")\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,      # 부스팅 단계 수\n",
    "    learning_rate=0.3,     # 학습률\n",
    "    max_depth=5,          # 트리의 최대 깊이\n",
    "    min_samples_split=2,   # 내부 노드 분할에 필요한 최소 샘플 수\n",
    "    min_samples_leaf=1,    # 리프 노드가 되기 위한 최소 샘플 수\n",
    "    subsample=1.0,        # 각 단계마다 사용할 샘플의 비율\n",
    "    random_state=42       # 재현성을 위한 랜덤 시드\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e1daddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree : (0.6385986478180701, 0.597051597051597)\n",
      "fores : (0.9096496619545175, 0.8353808353808354)\n",
      "fores : (1.0, 0.9434889434889435)\n"
     ]
    }
   ],
   "source": [
    "tree.fit(x_train, y_train)\n",
    "print(f'tree : {tree.score(x_train, y_train), tree.score(x_test, y_test)}')\n",
    "\n",
    "fores.fit(x_train, y_train)\n",
    "print(f'fores : {fores.score(x_train, y_train), fores.score(x_test, y_test)}')\n",
    "\n",
    "gb.fit(x_train, y_train)\n",
    "print(f'gb : {gb.score(x_train, y_train), gb.score(x_test, y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bb830869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 파라미터: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(f\"최적 파라미터: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8ee55fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test \\\n",
    "    = train_test_split(train_data, train_target, test_size=0.2\n",
    "                       ,stratify=train_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "62adbb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "# RegexpTokenizer + stopwords + PosterStemmer\n",
    "english_stops = set(stopwords.words('english'))\n",
    "regtok = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    toks = regtok.tokenize(text.lower())\n",
    "    toks = [t for t in toks if t not in english_stops]\n",
    "    tokes = [PorterStemmer().stem(t) for t in toks]\n",
    "    return tokes\n",
    "tfidf_custom =TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf_c = tfidf_custom.fit_transform(x_train)\n",
    "x_test_tfidf_c  = tfidf_custom.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eab9433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_c : (0.9858635525507068, 0.9385749385749386)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_tfidf_c, y_train)\n",
    "print(f'lr_c : {lr_c.score(x_train_tfidf_c, y_train), lr_c.score(x_test_tfidf_c, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "23f7d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_c : (0.9956976029502151, 0.9557739557739557)\n"
     ]
    }
   ],
   "source": [
    "# n-gram 실험 1,2 1,3\n",
    "# 성능향상 기대 연속된 단어패턴 포착\n",
    "tfidf_12 = TfidfVectorizer(token_pattern=r\"[\\w']{3,}\"\n",
    "                        , stop_words=stopwords.words('english')\n",
    "                        , min_df=2, max_df=0.5\n",
    "                        )\n",
    "\n",
    "x_train_12 = tfidf_12.fit_transform(x_train)\n",
    "x_test_12 = tfidf_12.transform(x_test)\n",
    "\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_12, y_train)\n",
    "\n",
    "print(f'lr_c : {lr_c.score(x_train_12, y_train), lr_c.score(x_test_12, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc40c8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  ...   title\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만  ...  인피니티 워\n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.  ...  인피니티 워\n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...  ...  인피니티 워\n",
       "3                                이 정도면 볼만하다고 할 수 있음!  ...  인피니티 워\n",
       "4                                               재미있다  ...  인피니티 워\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 처리 Konlpy \n",
    "# 품사 기반 태깅 tokenizer Noun Verb Adjetive\n",
    "# 데이터 로딩\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d7b0dc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['인피니티 워', '라라랜드', '곤지암', '신과함께', '범죄도시', '택시운전사', '코코'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecf652ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train, y_test = train_test_split(df.review, df.title, stratify=df.title, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ab7a98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a9a353f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# simple version\n",
    "tfidf=TfidfVectorizer(tokenizer=okt.nouns, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf=tfidf.fit_transform(x_train)\n",
    "x_test_tfidf=tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "51b9a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7574702886247878 0.6896434634974533\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train_tfidf, y_train)\n",
    "print(clf.score(x_train_tfidf, y_train), clf.score(x_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dbc13d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7824278438030561 0.7144312393887946\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    target = ['Noun','Verb','Adjective']\n",
    "    return [w for w,tag in okt.pos(text, norm=True, stem=True) if tag in target]\n",
    "\n",
    "tfidf=TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf=tfidf.fit_transform(x_train)\n",
    "x_test_tfidf=tfidf.transform(x_test)\n",
    "clf2 = LogisticRegression(max_iter=1000)\n",
    "clf2.fit(x_train_tfidf, y_train)\n",
    "print(clf2.score(x_train_tfidf, y_train), clf2.score(x_test_tfidf, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
