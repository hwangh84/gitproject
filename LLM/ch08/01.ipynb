{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851bf491",
   "metadata": {},
   "source": [
    "### ÏûêÏó∞Ïñ¥ Í∞êÏÑ±Î∂ÑÏÑù\n",
    "    -Í∞êÏÑ±ÏÇ¨Ï†Ñ Í∏∞Î∞ò : ÎØ∏Î¶¨ Ï†ïÏùòÎêú Í∞êÏÑ± Îã®Ïñ¥ ÏÇ¨Ï†Ñ ÏÇ¨Ïö©(Í∑úÏπô Í∏∞Î∞ò)\n",
    "        - TextBlob,  AFINNm VADER\n",
    "    -Î®∏Ïã†Îü¨Îãù Í∏∞Î∞ò : Îç∞Ïù¥ÌÑ∞Î°úÎ∂ÄÌÑ∞ Ìå®ÌÑ¥ ÌïôÏäµ(ÌÜµÍ≥ÑÍ∏∞Î∞ò)\n",
    "        - TF-IDF Î≤°ÌÑ∞Ìôî\n",
    "        - ÏÑ†ÌòïÌöåÍ∑Ä\n",
    "        - Î°úÏßÄÏä§Ìã±ÌöåÍ∑Ä\n",
    "        - F1 SCORE,Recison, Recall -> classification report \n",
    "### ÏÇ¨Ïö© Îç∞Ïù¥ÌÑ∞ \n",
    "    - NLTK ÏòÅÌôî Î¶¨Î∑∞(2000Í∞ú)\n",
    "    - Îã§ÏùåÏòÅÌôîÎ¶¨Î∑∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa69c4a",
   "metadata": {},
   "source": [
    "### ÏïåÍ≥†Î¶¨Ï¶ò\n",
    "    - TextBlob ÏÇ¨Ï†ÑÍ∏∞Î∞ò Í∞êÏÑ±Î∂ÑÏÑù\n",
    "    - AFINN    Í∞êÏ†ï Ï†êÏàò Îß§Ìïë\n",
    "    - VADER*(Valance Aware Dictionary) ÏÜåÏÖúÎØ∏ÎîîÏñ¥ ÏµúÏ†ÅÌôî Í∞êÏÑ±Î∂ÑÏÑù\n",
    "    - TF-IDF ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî\n",
    "    - Multinormal Native Bayes  ÌôïÎ•† Í∏∞Î∞ò Î∂ÑÎ•ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b5b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Retrieving notices: done\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - textblob\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    textblob-0.19.0            |  py311hbc747e5_0         686 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         686 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  textblob           pkgs/main/win-64::textblob-0.19.0-py311hbc747e5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "textblob-0.19.0      | 686 KB    |            |   0% \n",
      "textblob-0.19.0      | 686 KB    | 2          |   2% \n",
      "textblob-0.19.0      | 686 KB    | #8         |  19% \n",
      "textblob-0.19.0      | 686 KB    | #####5     |  56% \n",
      "textblob-0.19.0      | 686 KB    | ########## | 100% \n",
      "textblob-0.19.0      | 686 KB    | ########## | 100% \n",
      "textblob-0.19.0      | 686 KB    | ########## | 100% \n",
      "                                                     \n",
      " done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %conda install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d93203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob\n",
    "# Ïù¥ ÏòÅÌôîÎäî Ï†ïÎßê Ï¢ãÍ≥† Ïû¨ÎØ∏ÏûàÎã§\n",
    "    # Ï¢ãÎã§ +1(Í∏çÏ†ï)\n",
    "    # Ïû¨ÎØ∏ÏûàÎã§ +1(Í∏çÏ†ï)\n",
    "    # +2 > 0 --> Í∏çÏ†ï(pos) Î∂ÑÎ•ò\n",
    "# Polarity(Í∑πÏÑ±ÎèÑ) (Í∏çÏ†ïÎã®Ïñ¥Ïàò Í∞úÏàò - Î∂ÄÏ†ïÎã®Ïñ¥ Í∞úÏàò) / Ï†ÑÏ≤¥ Îã®Ïñ¥ Í∞úÏàò\n",
    "# -1.0 ~ +1.0\n",
    "# 0 Ï§ëÎ¶Ω\n",
    "# Subjectivity(Ï£ºÍ¥ÄÏÑ±) ÌèâÍ∞ÄÎåÄÏÉÅ Îã®Ïñ¥ ÎπÑÏú®\n",
    "# 0.0 ~ 1.0\n",
    "# 0: Í∞ùÍ¥ÄÏ†Å 1: Ï£ºÍ¥ÄÏ†Å\n",
    "\n",
    "# Î¨∏Îß• Î¨¥ÏãúÌïòÍ≥† Îã®Ïñ¥ Í∑πÏÑ±Îßå Í≥†Î†§\n",
    "# Ïù¥ ÏòÅÌôîÎäî ÎÇòÏÅòÏßÄ ÏïäÎã§ -> ÎÇòÏÅòÎã§(-) ÏïäÎã§(-)Î°ú Ïù∏Ïãù\n",
    "# Ïû•Ï†ê : Îπ†Î•∏ ÏÜçÎèÑ, ÌïôÏäµ Î∂àÌïÑÏöî\n",
    "# ÏÇ¨Ïö© : Ïã§ÏãúÍ∞Ñ Í∞êÏÑ±Î∂ÑÏÑù, Ïä§Ìä∏Î¶¨Î∞çÎç∞Ïù¥ÌÑ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88644c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"TextBlob is amazingly simple to use.\"), Sentence(\"What a wonderful library for NLP!\")]\n",
      "['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'wonderful', 'library', 'for', 'NLP']\n",
      "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('amazingly', 'RB'), ('simple', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('What', 'WP'), ('a', 'DT'), ('wonderful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP')]\n",
      "['textblob', 'wonderful library', 'nlp']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "polarity : 0.5\n",
      "subjectivity : 0.6785714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\khh11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob   # TextBlob: Í∞ÑÎã®Ìïú NLP(ÌÜ†ÌÅ∞Ìôî/ÌíàÏÇ¨/Î™ÖÏÇ¨Íµ¨/Í∞êÏÑ± Îì±) Ïú†Ìã∏ Ï†úÍ≥µ\n",
    "import nltk                    # NLTK: ÏΩîÌçºÏä§/ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä/ÌÉúÍ±∞ Îì± NLP ÎèÑÍµ¨\n",
    "# NLTK Î¶¨ÏÜåÏä§(brown ÏΩîÌçºÏä§)Î•º Îã§Ïö¥Î°úÎìú. ÌÉúÍ±∞ÎÇò ÏùºÎ∂Ä Í∏∞Îä•Ïù¥ Ïù¥ ÏΩîÌçºÏä§Ïóê ÏùòÏ°¥Ìï† Ïàò ÏûàÏùå.\n",
    "# (Ìïú Î≤àÎßå Ïã§ÌñâÌïòÎ©¥ Îê®, Ïò§ÌîÑÎùºÏù∏ ÌôòÍ≤ΩÏù¥Î©¥ ÏÉùÎûµ Í∞ÄÎä•)\n",
    "nltk.download('brown')\n",
    "\n",
    "# Î∂ÑÏÑùÌï† ÏÉòÌîå ÌÖçÏä§Ìä∏(Ïó¨Îü¨ Î¨∏Ïû• Í∞ÄÎä•)\n",
    "text = \"TextBlob is amazingly simple to use. What a wonderful library for NLP!\"\n",
    "\n",
    "# TextBlob Í∞ùÏ≤¥ ÏÉùÏÑ±:\n",
    "# - ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Î¨∏Ïû• Î∂ÑÎ¶¨, ÌÜ†ÌÅ∞Ìôî, ÌíàÏÇ¨ ÌÉúÍπÖ, Î™ÖÏÇ¨Íµ¨ Ï∂îÏ∂ú Îì±ÏùÑ ÏßÄÏõê\n",
    "# - ÏòÅÏñ¥Ïóê ÏµúÏ†ÅÌôîÎêòÏñ¥ ÏûàÏùå\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# blob.sentences: Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÎ¶¨Îêú Sentence Í∞ùÏ≤¥Îì§Ïùò Î¶¨Ïä§Ìä∏ Ï∂úÎ†•\n",
    "# Ïòà: [Sentence(\"TextBlob is amazingly simple to use.\"), Sentence(\"What a wonderful library for NLP!\")]\n",
    "print(blob.sentences)\n",
    "\n",
    "# blob.words: Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ÏóêÏÑú Ï∂îÏ∂úÌïú Îã®Ïñ¥ ÌÜ†ÌÅ∞(WordList) Ï∂úÎ†•\n",
    "# Íµ¨ÎëêÏ†ê Îì±ÏùÄ Ï†úÍ±∞Îêú ÌÜ†ÌÅ∞ Î¶¨Ïä§Ìä∏Í∞Ä Î∞òÌôòÎê®\n",
    "print(blob.words)\n",
    "\n",
    "# blob.tags: (Îã®Ïñ¥, ÌíàÏÇ¨ÌÉúÍ∑∏) ÌäúÌîåÏùò Î¶¨Ïä§Ìä∏Î•º Î∞òÌôò (ÌíàÏÇ¨ ÌÉúÍ∑∏Îäî Penn Treebank ÌÉúÍ∑∏ Îì±)\n",
    "# Ïòà: [('TextBlob', 'NNP'), ('is', 'VBZ'), ...]\n",
    "print(blob.tags)\n",
    "\n",
    "# blob.noun_phrases: Î™ÖÏÇ¨Íµ¨ Ï∂îÏ∂ú Í≤∞Í≥º(Í∞ÑÎã®Ìïú Í∑úÏπô/ÌÉúÍ±∞ Í∏∞Î∞ò)\n",
    "# Î™ÖÏÇ¨Íµ¨Îäî lower-caseÎ°ú Ï†ïÍ∑úÌôîÎêòÏñ¥ Î∞òÌôòÎêòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏùå\n",
    "print(blob.noun_phrases)\n",
    "\n",
    "# Í∞ÄÏãúÏÑ±ÏùÑ ÏúÑÌïú Íµ¨Î∂ÑÏÑ†\n",
    "print('-'*100)\n",
    "\n",
    "# Í∞êÏÑ±Î∂ÑÏÑù: blob.sentimentÎäî namedtuple(Sentiment(polarity, subjectivity)) Î∞òÌôò\n",
    "# - polarity: -1.0(Îß§Ïö∞ Î∂ÄÏ†ï) ~ +1.0(Îß§Ïö∞ Í∏çÏ†ï)\n",
    "# - subjectivity: 0.0(Í∞ùÍ¥ÄÏ†Å) ~ 1.0(Ï£ºÍ¥ÄÏ†Å)\n",
    "# TextBlobÏùò Í∏∞Î≥∏ Sentiment Î∂ÑÏÑùÍ∏∞Îäî Pattern Í∏∞Î∞ò Î∂ÑÏÑùÍ∏∞ÎÇò Í∞ÑÎã®Ìïú Í∑úÏπôÏùÑ ÏÇ¨Ïö©ÌïòÎØÄÎ°ú\n",
    "# Î¨∏Îß•(Î∂ÄÏ†ïÏñ¥ Ï≤òÎ¶¨ Îì±)Ïóê ÏïΩÌï† Ïàò ÏûàÏùå(Ïòà: \"not bad\" Ï≤òÎ¶¨ ÎØ∏Ìù°).\n",
    "print(f'polarity : {blob.sentiment.polarity}')\n",
    "print(f'subjectivity : {blob.sentiment.subjectivity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96cf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN(Lexicon-Based) Í∞êÏÑ±ÏÇ¨Ï†Ñ\n",
    "# Í∞Å Îã®Ïñ¥Ïùò -5 ~ +5Ïùò Ï†êÏàòÎ•º Î∂ÄÏó¨ÌïòÍ≥† Ìï©ÏÇ∞\n",
    "# Ïù¥ ÏòÅÌôîÎäî Ï¢ãÏßÄÎßå Ï¢ãÏßÄ ÏïäÏùÄ Î∂ÄÎ∂ÑÎèÑ ÏûàÎã§\n",
    "    # Ï¢ãÎã§ +3 Ï¢ãÎã§ +3 ÎÇòÏÅòÎã§ -3 = +3 > 0 Í∏çÏ†ï\n",
    "# score = sum(word_sentiment_value)\n",
    "# Î∂ÑÎ•òÍ∑úÏπô score 0> Í∏çÏ†ï score 0< Î∂ÄÏ†ï\n",
    "# Ïù¥Î™®Ìã∞ÏΩò ÏßÄÏõê\n",
    "# Í∞êÎèÑÌëúÌòÑ Ïù∏Ïãù very, really Îì±\n",
    "\n",
    "# Í∞êÎèÑ ÏàòÏ†ïÏûê(intensifiers)\n",
    "    # Îß§Ïö∞Ï¢ãÎã§ = 1.5X(Ï¢ãÎã§Ïùò Ï†êÏàò)\n",
    "\n",
    "# AFINN vs TextBlob\n",
    "# AFINN : Îçî Ï†ïÌôïÌïú Ï†êÏàò Îß§Ìïë\n",
    "# TextBlob : Îçî ÏùºÎ∞òÏ†ÅÏù∏ Ï†ëÍ∑º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91247f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py): started\n",
      "  Building wheel for afinn (setup.py): finished with status 'done'\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53479 sha256=37d8cdb106cfca8dc6b40da6ffc5beba130f7a7fbf61affba100f0d26f320f51\n",
      "  Stored in directory: c:\\users\\khh11\\appdata\\local\\pip\\cache\\wheels\\ee\\d3\\a0\\f9255ebac29886acb1c28b35b37523f6399677fa06be379f25\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'afinn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'afinn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7806c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "text1 = \"TextBlob is amazingly simple to use.\"\n",
    "text2 = \"What a wonderful library for NLP!\"\n",
    "score1 = af.score(text1)\n",
    "score2 = af.score(text2)\n",
    "\n",
    "score1, score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER ÏÜåÏÖú ÎØ∏ÎîîÏñ¥ ÌÖçÏä§Ìä∏Ïóê ÏµúÏ†ÅÌôî\n",
    "# Ïù¥ ÏòÅÌôîÎäî Ï†ïÎßê ÌõåÎ•≠Ìï¥!!!\n",
    "# ÌõåÎ•≠ÌïòÎã§ (Í∏∞Î≥∏) + 0.7 Ï†ïÎßêÏ†ïÎßê (Í∞ïÏ°∞)x1.5\n",
    "# !!! (Î¨∏Ïû•Î∂ÄÌò∏Í∞ïÏ°∞) x1.2\n",
    "# 4Í∞úÏùò Í∞êÏ†ï ÏßÄÏàò\n",
    "    # positive  Í∏çÏ†ïÌôïÎ•† 0~1\n",
    "    # negative  Î∂ÄÏ†ïÌôïÎ•†\n",
    "    # neutral   Ï§ëÎ¶Ω ÌôïÎ•†\n",
    "    # compound  Ï¢ÖÌï©Ï†êÏàò -1 ~ 1\n",
    "# score = compound_score / sqrt(compound_score**2 + 0.0625)\n",
    "# score >= 0.05 Í∏çÏ†ï\n",
    "# score <= 0.05 Î∂ÄÏ†ï\n",
    "# Í∑∏ ÏÇ¨Ïù¥Îäî Ï§ëÎ¶Ω\n",
    "# ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ AMAZING amazing Îã§Î•∏ Ï†êÏàò\n",
    "# :) Í∏çÏ†ï  :-( Î∂ÄÏ†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de46739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\khh11\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "092fde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏Ïû• : I love this product! It's absolutely amazing üòç\n",
      "Ï†êÏàò : {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
      "Î¨∏Ïû• : This is the worst movie I've ever seen...\n",
      "Ï†êÏàò : {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      "Î¨∏Ïû• : The food was okay, not great but not bad either.\n",
      "Ï†êÏàò : {'neg': 0.149, 'neu': 0.487, 'pos': 0.364, 'compound': 0.4728}\n",
      "Î¨∏Ïû• : I‚Äôm REALLY happy with the results!!!\n",
      "Ï†êÏàò : {'neg': 0.0, 'neu': 0.472, 'pos': 0.528, 'compound': 0.7651}\n",
      "Î¨∏Ïû• : Not good at all. I‚Äôm disappointed.\n",
      "Ï†êÏàò : {'neg': 0.579, 'neu': 0.421, 'pos': 0.0, 'compound': -0.6711}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyer = SentimentIntensityAnalyzer()\n",
    "sentences = [\n",
    "    \"I love this product! It's absolutely amazing üòç\",\n",
    "    \"This is the worst movie I've ever seen...\",\n",
    "    \"The food was okay, not great but not bad either.\",\n",
    "    \"I‚Äôm REALLY happy with the results!!!\",\n",
    "    \"Not good at all. I‚Äôm disappointed.\",\n",
    "]\n",
    "for s in sentences:\n",
    "    scores = analyer.polarity_scores(s)\n",
    "    print(f'Î¨∏Ïû• : {s}')\n",
    "    print(f'Ï†êÏàò : {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4b507ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# nltk Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# ÏòÅÌôî Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "fileids = movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "863619e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 1000, 900)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids[100:]]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids[100:]]\n",
    "len(reviews), categories.count('pos'), categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10aaffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ïÌôïÎèÑ : 0.6\n"
     ]
    }
   ],
   "source": [
    "# 1 TextBlob\n",
    "def sentiment_testblob(docs):\n",
    "    return ['pos' if TextBlob(doc).sentiment.polarity>0 else 'neg' for doc in docs]\n",
    "predictions_textblob = sentiment_testblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories, predictions_textblob)\n",
    "print(f'Ï†ïÌôïÎèÑ : {accuracy_textblob:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4736ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ïÌôïÎèÑ : 0.7\n"
     ]
    }
   ],
   "source": [
    "# 2 AFINN\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    return ['pos' if afn.score(doc) > 0 else 'neg' for doc in docs]\n",
    "predictions = sentiment_afinn(reviews)\n",
    "accuracy = accuracy_score(categories ,predictions)\n",
    "print(f'Ï†ïÌôïÎèÑ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25177fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ïÌôïÎèÑ : 0.6\n"
     ]
    }
   ],
   "source": [
    "# 3 VADER\n",
    "def sentiment_vader(docs):\n",
    "    analyer = SentimentIntensityAnalyzer()\n",
    "    return ['pos' if analyer.polarity_scores(doc)['compound'] > 0 else 'neg' for doc in docs]\n",
    "predictions = sentiment_vader(reviews)\n",
    "accuracy = accuracy_score(categories ,predictions)\n",
    "print(f'Ï†ïÌôïÎèÑ : {accuracy:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6e060aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î®∏Ïã†Îü¨Îãù Í∏∞Î∞ò Í∞êÏÑ±Î∂ÑÏÑù\n",
    "# \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎÇòÏù¥Î∏å Î≤†Ïù¥Ï¶à\n",
    "\n",
    "# Î≤†Ïù¥Ï¶à Ï†ïÎ¶¨\n",
    "# \"Ï¢ãÎã§\" Îã®Ïñ¥Î•º Î≥∏ ÌõÑ Ïù¥ Î¶¨Î∑∞Í∞Ä Í∏çÏ†ïÏùº ÌôïÎ•†\n",
    "# $$P(Í∏çÏ†ï | \"Ï¢ãÎã§\") = \\frac{P(\"Ï¢ãÎã§\" | Í∏çÏ†ï) \\times P(Í∏çÏ†ï)}{P(\"Ï¢ãÎã§\")}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b24ed",
   "metadata": {},
   "source": [
    "$$P(Í∏çÏ†ï | \"Ï¢ãÎã§\") = \\frac{P(\"Ï¢ãÎã§\" | Í∏çÏ†ï) \\times P(Í∏çÏ†ï)}{P(\"Ï¢ãÎã§\")}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1cfcbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 1520)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "dataset = train_test_split(reviews, categories, test_size=0.2, random_state=42, stratify=categories)\n",
    "len(dataset[0]), len(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8676e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf Î≤°ÌÑ∞Ìôî\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "x_test  = vectorizer.transform(dataset[1])\n",
    "\n",
    "y_train = dataset[2]\n",
    "y_test  = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fb5fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.63      0.71       180\n",
      "         pos       0.72      0.87      0.79       200\n",
      "\n",
      "    accuracy                           0.76       380\n",
      "   macro avg       0.77      0.75      0.75       380\n",
      "weighted avg       0.77      0.76      0.75       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. mnb\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train, y_train)\n",
    "predict = mnb_clf.predict(x_test)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d4f5348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.84      0.70      0.76       180\n",
      "         pos       0.77      0.88      0.82       200\n",
      "\n",
      "    accuracy                           0.79       380\n",
      "   macro avg       0.80      0.79      0.79       380\n",
      "weighted avg       0.80      0.79      0.79       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logisticregression\n",
    "logi = LogisticRegression()\n",
    "logi.fit(x_train, y_train)\n",
    "predict = logi.predict(x_test)\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9df6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏÑ±Îä•Ìñ•ÏÉÅ\n",
    "# ÏÜåÎ¨∏ÏûêÎ≥ÄÌôò - Ïó∞ÏÜçÎêú Î¨∏ÏûêÏó¥ Ï§ëÏóê 3Í∏ÄÏûê Ïù¥ÏÉÅ - Ïñ¥Í∞ÑÏ∂îÏ∂ú(ÌòïÌÉúÏÜå Î∂ÑÏÑù) - Î∂àÏö©Ïñ¥ Ï†úÍ±∞\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b4c9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khh11\\miniconda3\\envs\\pyt_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "    tokens =tokenizer.tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [porter.stem(token) for token in tokens if token not in stop_words ]\n",
    "vector = TfidfVectorizer(\n",
    "     tokenizer= custom_tokenizer\n",
    "    ,max_features=1000\n",
    "    ,min_df=5\n",
    "    ,max_df=0.5\n",
    "    ,token_pattern=r\"[\\w']{3,}\"\n",
    ")\n",
    "x_train = vector.fit_transform(dataset[0])\n",
    "x_test  = vector.transform(dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "671d4a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.0677247],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]], shape=(1520, 1000))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dcdc5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.fit(x_train, y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    print(f'{str(model).split(\"(\")[0]}\\n {classification_report(y_test, predict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0518c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.73      0.79       180\n",
      "         pos       0.79      0.91      0.84       200\n",
      "\n",
      "    accuracy                           0.82       380\n",
      "   macro avg       0.83      0.82      0.82       380\n",
      "weighted avg       0.83      0.82      0.82       380\n",
      "\n",
      "MultinomialNB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.73      0.78       180\n",
      "         pos       0.78      0.86      0.82       200\n",
      "\n",
      "    accuracy                           0.80       380\n",
      "   macro avg       0.80      0.80      0.80       380\n",
      "weighted avg       0.80      0.80      0.80       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(LogisticRegression(max_iter=1000))\n",
    "evaluate_model(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30c80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
